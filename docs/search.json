[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Updated Resume",
    "section": "",
    "text": "I am open for new opportunities both for remote as well as hybrid setup. Kindly reach out at koushikkhan38@gmail.com if you think I might be a good fit.\nA copy of my updated resume can be downloaded from here."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMake More Sense of Language Through Embeddings\n\n\n\n\n\n\ndeep learning\n\n\n\nRepresenting texts in a better way that neural networks understand\n\n\n\n\n\nMay 1, 2023\n\n\nKoushik Khan\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Calculation Through Computational Graph\n\n\n\n\n\n\ndeep learning\n\n\n\nThe way, deep neural networks learn\n\n\n\n\n\nApr 22, 2023\n\n\nKoushik Khan\n\n\n\n\n\n\n\n\n\n\n\n\nBash Commands to Get Started on Linux Clouds\n\n\n\n\n\n\nlinux\n\n\ncloud\n\n\n\nPart 2: Advanced Tools for Commandline\n\n\n\n\n\nMar 26, 2023\n\n\nKoushik Khan\n\n\n\n\n\n\n\n\n\n\n\n\nBash Commands to Get Started on Linux Clouds\n\n\n\n\n\n\nlinux\n\n\ncloud\n\n\n\nPart 1: Basics\n\n\n\n\n\nMar 22, 2023\n\n\nKoushik Khan\n\n\n\n\n\n\n\n\n\n\n\n\nTesting of Hypotheses: A Quick Refresher\n\n\n\n\n\n\nstatistics\n\n\n\nPart 1: Basics\n\n\n\n\n\nMar 19, 2023\n\n\nKoushik Khan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills and Expertises",
    "section": "",
    "text": "Along the journey in my career, I have worked on various projects, all of them may not be worth of mentioning. I have handpicked few of them here along with the tools and methodologies used."
  },
  {
    "objectID": "skills.html#academics",
    "href": "skills.html#academics",
    "title": "Skills and Expertises",
    "section": " Academics",
    "text": "Academics\n\nDigitization of class notes, preparation of project reports\n\n Tools\n\n\\LaTeX\nR with Sweave and knitr packages\n\n\nPerforming computations for ‚ÄòDesign of Experiments‚Äô problems\n\n Tools\n\nC programming"
  },
  {
    "objectID": "skills.html#bcause-enterprise-pvt.-limited-2015-2016",
    "href": "skills.html#bcause-enterprise-pvt.-limited-2015-2016",
    "title": "Skills and Expertises",
    "section": " BCausE Enterprise Pvt. Limited (2015-2016)",
    "text": "BCausE Enterprise Pvt. Limited (2015-2016)\n\nDevelopment of customer segmentation model\n\n Tools\n\nClustering\nPython\nMongoDB\nPython Flask, used for developing REST API"
  },
  {
    "objectID": "skills.html#ibm-india-pvt.-ltd.-2016-2021",
    "href": "skills.html#ibm-india-pvt.-ltd.-2016-2021",
    "title": "Skills and Expertises",
    "section": " IBM India Pvt. Ltd.¬†(2016-2021)",
    "text": "IBM India Pvt. Ltd.¬†(2016-2021)\n\nAnalysing customer complaints (a.k.a tickets) from various sources and predicting root causes using clustering and text classification respectively\n\n Tools\n\nMachine Learning\nPython\nSQL\nShellScript\nPython Flask, used for developing REST API\nDocker\nKubernetes, used for deployment\n\n\nServing search queries to customers by designing a small scale search engine built on customer Feedbacks and FAQs using information retrieval algorithm\n\n Tools\n\nSingular value decomposition\nPython, Pandas and Numpy\n\n\nUnderstanding the effects of various factors like advertisement, promotion, weather on sales of several commodities, analysing ROI year-on-year basis and forecasting the furure sales using Marketing Mix Modeling\n\n Tools\n\nSales and ad data\nRegression Techniques\nR, Dplyr\nSQL"
  },
  {
    "objectID": "skills.html#sp-global-inc.-2021-present",
    "href": "skills.html#sp-global-inc.-2021-present",
    "title": "Skills and Expertises",
    "section": " S&P Global Inc.¬†(2021-Present)",
    "text": "S&P Global Inc.¬†(2021-Present)\n\nDevelopment of data driven products in shipping analytics\n\n Tools\n\nPython, PySpark, Polars\nExcel, used for reporting\nAmazon AWS\n\n\nTracking green house gas emission from ships\n\n Tools\n\nShipping data\nPython, PySpark, Polars\nExcel, used for reporting\nAmazon AWS\n\n\nBuilding data pipelines in Azure DevOps\n\n Tools\n\nR, Python, PySpark, Polars\nMicrosoft Azure as DevOps platform"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Here are some personal projects that I have worked on so far while learning various things. I wish to bring many more in future.\n\nMasters Thesis: Joint Modelling of Longitudinal and Time-to-event data - Thesis, Slide, Data\nDevelopment of my Portfolio [web development, technical writing] - GitHub\nWriting gradient descent from scratch [computing, python] - GitHub\nA simple pipeline to package & deploy a machine learning app [ml, deployment, docker] - GitHub\nAutomating the boring stuffs after installing a linux distro [linux, shellscript] - GitHub\nA not so short introduction to object oriented programming using R [technical writing] - Medium\nStock price forecasting with LSTM network [forecasting, deep learning] - Colab\nPlaying with word2vec [neural net, embedding] - Colab\nA simple automation plan for time series forecasting: from data acquisition to model building and making forecast [time series, automation, rest api] - GitHub"
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html",
    "href": "posts/2023-04-22-autograd/index.html",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "",
    "text": "Photo by Alina Grubnyak on Unsplash"
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#derivative-for-different-type-of-input-mappings",
    "href": "posts/2023-04-22-autograd/index.html#derivative-for-different-type-of-input-mappings",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "Derivative for different type of input mappings",
    "text": "Derivative for different type of input mappings\n\nFunctions of type f: \\mathbb{R}^n \\to \\mathbb{R} - vector in, scalar out\nSuch functions take vector (s) as input (s) and return a scalar. The mean-squared-error and cross-entropy loss functions are a couple of examples of such functions that are heavily used in neural network setup.\n\n\n\n\n\n\nmean squared error (mse) loss for regression setup\n\n\n\nSuppose you have a continuous target variable (a.k.a dependent variable) Y corresponding to a feature (a.k.a independent variable) X for a regression problem.\nLet us take N as the total number of such pairs (i.e.¬†examples) (x_i, y_i)_{i=1}^N in our data. Now, at the end of the learning process of a neural network, we are about to get N predictions which is another set of values \\hat{y}_{i=1}^N.\nTo measure the goodness of fit of the predictions w.r.t to actual target values (y_i ‚Äôs), typically a mse loss function is used. This is defined as below:\n\nmse(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\nWe expect the mse value to be close to zero (or some predefined threshold depending on the use case and problem setup) to ensure the correctness of the prediction.\n\n\n\n\n\n\n\n\ncross entropy loss for classification setup\n\n\n\nConsider a binary classification problem with X as the single feature variable and Y as the class labels which can take a value as either zero or one. This is also known as a binary logistic regression problem.\nAs before, we have a total of N examples (x_i, y_i)_{i=1}^N in our data, where \\forall i, y_i \\in \\left\\{0,1\\right\\}.\nBinary classification is the simplest form of classification problem, which will help us to understand the concept easily.\nSince, Y can take only two possible values, it‚Äôs safe to consider that Y is having a bernoulli distribution.\nAlso assume,\n\\begin{align}\n  Y &= 1 \\Rightarrow \\text{success} \\nonumber \\\\\n    &= 0 \\Rightarrow \\text{failure} \\nonumber\n\\end{align}\nHere we need one more quantity p as probability of having a success as part of the distribution of Y, it is an important attribute of the data that we have.\nNow, following the nature of bernoulli distribution, the probability of Y taking an outcome, can be expressed in a generic way as given below:\n\nP(Y = y) = p^y \\times (1-p)^{1-y}; \\text{where } Y \\in \\left\\{{0,1}\\right\\}\n\nNote that, putting y=1 in the above function will give you p. This function always gives a value in between zero and one. It is also known as the likelihood function given that the quantity p is unknown, but y is known.\nHaving defined the above expression, we are ready to look at the entire data. It is to be noted that each of the examples are independent of each other.\nNow, the next step is to find out the probability of having the target values themselves as y_1, y_2, \\ldots, y_N at some point of time that we are considering for the classification task.\nTo be specific, we would want to know that whichever process has generated the data, what would be the probability of generating this sample again. This combined probability is called likelihood.\nSince these Y_i‚Äôs are independent of each other, this likelihood (equivalent to joint probability) of Y_1=y_1, \\dots, Y_N=y_N is just the product of the individual likelihoods i.e.¬†\n\\begin{align}\n  P(Y_1=y_1, \\dots, Y_N=y_N) &= \\prod_{i=1}^N P(Y_i = y_i) \\nonumber \\\\\n                              &= \\prod_{i=1}^N p_i^{y_i} \\times (1-p_i)^{1-y_i} \\nonumber \\\\\n\\end{align}\nAt this stage, a log function is typically applied to scale the above quantity for future calculations. The log function has same pattern as the function on which it is applied to i.e.¬†mathematically speaking,\n\n\\text{if } \\forall a,b \\in \\mathbb{R}, a &gt; b  \\Rightarrow f(a) &gt; f(b), \\text{ then } log(f(a)) &gt; log(f(b))\n\nTherefore we have, \\begin{align}\n  log\\left[P(Y_1=y_1, \\dots, Y_N=y_N)\\right] &= \\sum_{i=1}^N y_i log(p_i) + (1-y_i) log(1-p_i) \\nonumber\n\\end{align}\nThe above quantity is a negative quantity as the log function returns negative values when the input is within zero and one and working with a negative quantity can be conceptually misleading. That is why it is usually multiplied by negative one to make it positive.\nThis positive quantity is often referred to as negative log likelihood (nll) and is considered as the loss function for this type of binary classification problems.\nHence, the negative log-likelihood loss function is finally expressed as,\n\nnll = -\\sum_{i=1}^N \\left\\{y_i log(p_i) + (1-y_i) log(1-p_i)\\right\\}\n\nIt should be noted that, the quantity p_i is usually estimated by the model (like a FFN for binary classification or logistic regression), whereas y_i being the true value in our data for the i^{th} sample.\nThe NLL loss function tries to measure the similarity between the distribution of the predicted values p and the distribution of actual values y (i.e.¬†labels).\nThe cross entropy loss function is just a generalisation of this simple negative log likelihood giving the similar kind of information for a prediction when the data has, say, K labels instead of just two.\nIt is expressed as,\n\n\\text{cross entropy loss} = - \\sum_{k=1}^K y_k log(\\hat{y}_k)\n\nWhere y_k‚Äôs are true probabilities (or give the true distribution of labels), whereas \\hat{y}_k‚Äôs are predicted probabilities (or give the predicted/estimated distribution of labels).\n\n\nThe vector of gradients of such a function f is defined as below:\n\n\\frac{df}{d\\vec{x}} = \\left(\\frac{df}{dx_1}, \\ldots, \\frac{df}{dx_n}\\right)'\n\n\n\nFunctions of type f: \\mathbb{R}^n \\to \\mathbb{R}^m - vector in, vector out\nSuch functions take vector (s) as input (s) and return another vector with different dimensions as output.\nA simple example can be a function f: \\mathbb{R}^2 \\to \\mathbb{R}^3:\n\ninput: \\vec{x} = \\left(x_1, x_2\\right) and a matrix of parameters \\mathbf{W}_{3 \\times 2}\noutput: \\vec{y} = f(\\vec{x}) = \\left(w_{11}x_1+w_{12}x_2, w_{21}x_1+w_{22}x_2, w_{31}x_1+w_{32}x_2\\right)\n\nIn this case, derivative of f(\\vec{x}) will be a matrix of dimension m \\times n, as given below:\n\\begin{align}\n    \\frac{d\\vec{y}}{d\\vec{x}} &= \\left(\\frac{d\\vec{y}}{dx_1}, \\ldots, \\frac{d\\vec{y}}{dx_n}\\right) \\nonumber \\\\\n    &= \\begin{pmatrix}\n            \\frac{dy_1}{dx_1} & \\frac{dy_1}{dx_2} & \\ldots & \\frac{dy_1}{dx_n} \\\\\n            \\frac{dy_2}{dx_1} & \\frac{dy_2}{dx_2} & \\ldots & \\frac{dy_2}{dx_n} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            \\frac{dy_m}{dx_1} & \\frac{dy_m}{dx_2} & \\ldots & \\frac{dy_m}{dx_n} \\\\\n        \\end{pmatrix} \\nonumber\n\\end{align}\nThis matrix is often referred to as jacobian matrix of \\vec{y} with respect to \\vec{x}."
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#the-chain-rule-of-differentiation",
    "href": "posts/2023-04-22-autograd/index.html#the-chain-rule-of-differentiation",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "The chain rule of differentiation",
    "text": "The chain rule of differentiation\nConsider the situation, where you have two different functions f(.) and g(.) defined as follows y = f(x) and z = g(y) = g\\left(f(x)\\right).\nWhile forming z, f(.) is the inner function and g(.) is the outer function. This is known as function composition as often denoted as g \\circ f(x).\nWe want to measure the rate of change in z w.r.t x i.e.¬†\\frac{dz}{dx}.\nThis derivative is computed as follows:\n\n\\frac{dz}{dx} = \\frac{dz}{dy} \\times \\frac{dy}{dx}\n\nand it is known as the chain rule of differentiation.\nHere, the actual derivative is computed in two steps,\n\nthe local derivative of z w.r.t y as z explicitly depends on y\nthe local derivative of y w.r.t x as y explicitly depends on x\n\nand these are multiplied together to form the actual derivative.\nWikipedia has an intuitive explanation to this as given below:\n\n\n\n\n\n\nUnderstanding chain rule\n\n\n\nIntuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.\nAs put by George F. Simmons: ‚Äúif a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 √ó 4 = 8 times as fast as the man.‚Äù\n\n\nThis chain rule is very important as it does all the heavy lifting for backpropagation."
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#partial-differentiation",
    "href": "posts/2023-04-22-autograd/index.html#partial-differentiation",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "Partial differentiation",
    "text": "Partial differentiation\nPartial differentiation is mainly applicable for a function that depends on several input variables simultaneously.\nFor example, we might have a function like f(x,y,z) = ax+by+cz+d representing a plane in a three dimensional coordinate system.\nIf we would like to measure the rate of change only along the x-axis, we would perform partial differentiation w.r.t x and by ignoring other inputs.\nUnlike the previous one (i.e.¬†complete derivative), partial derivative is denoted by \\frac{\\partial f}{\\partial x} and the definition is given below:\n\n\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x+h,y,z) - f(x,y,z)}{(x+h) - x}\n\n\n\n\n\n\n\nNote\n\n\n\nIn the neural network setup, we usually deal with complex functions that depend on several variables. Hence, it will be meaningful to use partial derivatives while demystifying the underlying operations."
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#differentiation-through-a-graph",
    "href": "posts/2023-04-22-autograd/index.html#differentiation-through-a-graph",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "Differentiation through a graph",
    "text": "Differentiation through a graph\nNeural networks learn (i.e.¬†update the parameters) using an iterative process where at the core it uses backpropagation algorithm to compute derivatives of a loss function w.r.t each node in the graph.\nLet us revisit the previously shown simple computational graph to understand it in a better way,\n\n\n\n\n\ngraph LR\n  a(a) --&gt; c(c = a+1)\n  a(a) --&gt; d(d = a+b)\n  b(b) --&gt; d(d = a+b)\n  c(c = a+1) --&gt; e(e = c * d)\n  d(d = a+b) --&gt; e(e = c * d)\n  e(e = c * d) --&gt; l(l = e+f)\n  f(f) --&gt; l(l = e+f)\n\n\n\n\n\n\nHere, we can assume l as the loss and we want to measure the rate of change of loss w.r.t each of the nodes (including loss node itself) in the backward direction (why backward? ‚Ä¶ I will explain later).\nThis whole computation is performed in multiple steps as follows:\n\nStep 1: rate of change of l w.r.t l itself i.e.¬†\\frac{\\partial l}{\\partial l} = 1\nStep 2: rate of change of l w.r.t e i.e.¬†\\frac{\\partial l}{\\partial e} = \\frac{\\partial l}{\\partial e} \\times \\frac{\\partial l}{\\partial l} = \\frac{\\partial (e+f)}{\\partial e} \\times \\frac{\\partial l}{\\partial l} = 1\nStep 3: rate of change of l w.r.t f i.e.¬†\\frac{\\partial l}{\\partial f} = \\frac{\\partial l}{\\partial f} \\times \\frac{\\partial l}{\\partial l} = \\frac{\\partial (e+f)}{\\partial f} \\times \\frac{\\partial l}{\\partial l} = 1\nStep 4: rate of change of l w.r.t c i.e.¬†\\frac{\\partial l}{\\partial c} = \\frac{\\partial e}{dc} \\times \\frac{\\partial l}{\\partial e} = \\frac{\\partial (c*d)}{\\partial c} \\times \\frac{\\partial l}{\\partial e} = d\nStep 5: rate of change of l w.r.t d i.e.¬†\\frac{\\partial l}{\\partial d} = \\frac{\\partial e}{\\partial d} \\times \\frac{\\partial l}{\\partial e} = \\frac{\\partial (c*d)}{\\partial d} \\times \\frac{\\partial l}{\\partial e} = c\nStep 6: rate of change of l w.r.t a i.e.¬†\\frac{\\partial l}{\\partial a}, this step can be broken into two parts as changing a, can change l either through c or through d, therefore,\n\n\\frac{\\partial l}{\\partial a} = \\underbrace{\\left( \\frac{\\partial c}{\\partial a} \\times \\frac{\\partial l}{\\partial c} \\right)}_{\\text{changes through c}} + \\underbrace{\\left( \\frac{\\partial d}{\\partial a} \\times \\frac{\\partial l}{\\partial d} \\right)}_{\\text{changes through d}} = d + c\n\nStep 7: rate of change of l w.r.t b i.e.¬†\\frac{\\partial l}{\\partial b} = \\frac{\\partial d}{\\partial b} \\times \\frac{\\partial l}{\\partial d} = 1 \\times c = c\n\nThe real benefit of computing derivatives (a.k.a gradients) in the backward direction is that it allows the learning algorithm to compute gradients for all the nodes with a single attempt.\nIf we would have started in the forward direction, then the learning algorithm would have travelled the entire graph for each of the input nodes at the very beginning. Unlike a simple graph like the above one, this forward mode differentiation is computationally very costly for a large network.\nPytorch, being a popular deep learning toolkit, has implemented a .backward() method for their neural network class to compute the derivatives for all the nodes (technically not all, only for them with requires_grad=True) in a computational graph with a single attempt."
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#important-takeaways",
    "href": "posts/2023-04-22-autograd/index.html#important-takeaways",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "Important takeaways",
    "text": "Important takeaways\nWhen we have a '+' operation in the graph, like the one below,\n\n\n\n\n\ngraph RL\n  subgraph input\n  x(x)\n  y(y)\n  end\n\n  subgraph output\n  z(z = x+y)\n  end\n\n  x(x) --&gt; z(z = x+y)-.-&gt;|gradient of z|x(x)\n  y(y) --&gt; z(z = x+y)-.-&gt;|gradient of z|y(y)\n\n\n\n\n\n\nComputing gradients with respect to the input nodes is fairly simple as the backward gradient computation just copies the gradient of the output node to all the input nodes.\nHowever, when we have a '\\times' operation in the graph, like the next one,\n\n\n\n\n\ngraph RL\n  x(x) --&gt; z(z = x*y)-.-&gt;|value of y * gradient of z|x(x)\n  y(y) --&gt; z(z = x*y)-.-&gt;|value of x * gradient of z|y(y)\n\n  subgraph input\n  x(x)\n  y(y)\n  end\n\n  subgraph output\n  z(z = x*y)\n  end\n\n\n\n\n\n\nThe gradient of one input node is just the product of the gradient of the output node and the value of the other input node.\n\n\n\n\n\n\nNote\n\n\n\n+ and \\times are the two building blocks powering up any mathematical computation."
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#sorting-nodes-before-performing-backpropagation",
    "href": "posts/2023-04-22-autograd/index.html#sorting-nodes-before-performing-backpropagation",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "Sorting nodes before performing backpropagation",
    "text": "Sorting nodes before performing backpropagation\n\n\n\n\n\ngraph LR\n  a(a) --&gt; c(c = a+b)\n  b(b) --&gt; c(c = a+b)\n\n\n\n\n\n\nIf we observe the above graph, it is clear that while performing backpropagation, computing gradient of any of the leaf nodes i.e.¬†\\left\\{a, b\\right\\} is possible only when gradient value for the node c is already available. This is just because of the chain rule we use.\nSo the fact is that we cannot choose a node randomly and try computing the gradient for it. We must compute the gradient of node c, which is essentially 1, then we can take either of node a or node b.\nNodes must be sorted in order to perform backpropagation. Topological Sort is a way to achieve this ordering before performing the backpropagation."
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#the-efficient-vector-jacobian-product",
    "href": "posts/2023-04-22-autograd/index.html#the-efficient-vector-jacobian-product",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "The efficient vector-jacobian product",
    "text": "The efficient vector-jacobian product\nConsider our tiny network:\n\n\n\n\n\n  graph LR\n    x1(x1)--&gt;|w11|y1(y1)\n    x2(x2)--&gt;|w12|y1(y1)\n    x1(x1)--&gt;|w21|y2(y2)\n    x2(x2)--&gt;|w22|y2(y2)\n    x1(x1)--&gt;|w31|y3(y3)\n    x2(x2)--&gt;|w32|y3(y3)\n    y1(y1)--&gt;l(l)\n    y2(y2)--&gt;l(l)\n    y3(y3)--&gt;l(l)\n\n    subgraph input&lt;br&gt;layer\n    x1(x1) \n    x2(x2)\n    end\n\n    subgraph hidden&lt;br&gt;layer\n    y1(y1) \n    y2(y2)\n    y3(y3)\n    end\n\n    subgraph output&lt;br&gt;layer\n    l(l)\n    end\n\n\n\n\n\n\nIt has only two input nodes, a single output node and in between them three additional nodes forming the hidden layer.\nThe mapping from input layer to hidden layer is supported by a matrix of parameters \\mathbf{W}_{3 \\times 2} and of course by a vector valued linear function, say, \\vec{f}.\nHidden layer values are formed using the equation below:\n\n\\begin{pmatrix}\n  y_1 \\\\\n  y_2 \\\\\n  y_3\n\\end{pmatrix} = \\begin{pmatrix}\n                  w_{11} & w_{12} \\\\\n                  w_{21} & w_{22} \\\\\n                  w_{31} & w_{32}\n                \\end{pmatrix} \\begin{pmatrix}\n                                x_1 \\\\\n                                x_2\n                              \\end{pmatrix}\n\nwhere w_{ij}; i=1(1)3, j=1(1)2 is the weight for the connection coming from j^{th} node in the input layer to i^{th} neuron in the hidden layer and consider a differentiable scalar valued function g(.) that combines hidden layer output values to form the final output l.\nComputing l from the given fixed set of input values and with the help of some values of the parameters, is called the forward pass.\nAt this stage, we can definitely compute the jacobian matrix of \\vec{f} w.r.t \\vec{x}, which is given below:\n\n\\mathbf{J} = \\begin{pmatrix}\n                \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\\n                \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} \\\\\n                \\frac{\\partial y_3}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_2}\n              \\end{pmatrix}\n\nNow, suppose \\vec{v} is the gradient vector of l i.e.¬†\\vec{v} = \\left(\\frac{dl}{dy_1}, \\frac{dl}{dy_2}, \\frac{dl}{dy_3}\\right)' and we will assume that we have already computed it. Strange? No problem, I will explain this assumption at the end of this section.\nNow, if we take the dot product of \\mathbf{J}^T and \\vec{v}, this is what we are going to get,\n\\begin{align}\n\\mathbf{J}^T \\cdot \\vec{v} &= \\begin{pmatrix}\n                                \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_1} \\\\\n                                \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_2} \\\\\n                              \\end{pmatrix} \\begin{pmatrix}\n                                  \\frac{\\partial l}{\\partial y_1} \\\\\n                                  \\frac{\\partial l}{\\partial y_2} \\\\\n                                  \\frac{\\partial l}{\\partial y_3} \\\\\n                              \\end{pmatrix} \\nonumber \\\\\n                           &= \\begin{pmatrix}\n                                  \\frac{\\partial l}{\\partial x_1} \\\\\n                                  \\frac{\\partial l}{\\partial x_2}\n                              \\end{pmatrix} \\nonumber \\\\\n                           &= \\frac{\\partial l}{\\partial \\vec{x}} \\nonumber\n\\end{align}\nIf we observe the dot product carefully, we will notice a series of chain rules being performed.\nTherefore, this vector-jacobian product helps us to compute the gradient of loss function w.r.t the nodes in another layer which, in reality, might be far behind the output layer in the computational graph and it does so with the help of a known gradient vector for the layer right next to it.\nThe intermediate gradient vector has been considered to be computed beforehand because while performing reverse mode differentiation, we must know the gradient of the output node before computing the gradients of the input nodes which have defined the output node itself and we have seen it here where we have started taking derivatives from the output node.\nHere \\vec{y} is the output of f(\\vec{x}), so we can safely take this assumption to understand the theory.\nNow, the next part is to compute \\frac{\\partial l}{\\partial \\mathbf{W}} which can again be done using,\n\n\\frac{\\partial l}{\\partial \\mathbf{W}} = \\frac{\\partial \\vec{y}}{\\partial \\mathbf{W}} \\times \\frac{\\partial l}{\\partial \\vec{y}}\n\nHopefully, it gives us an idea to implement automatic differentiation (autograd) for a large computational graph."
  },
  {
    "objectID": "posts/2023-04-22-autograd/index.html#the-training-loop",
    "href": "posts/2023-04-22-autograd/index.html#the-training-loop",
    "title": "Gradient Calculation Through Computational Graph",
    "section": "The training loop",
    "text": "The training loop\nThe last and one of the important parts of training a neural network is the training loop. Technically it is a for loop where in each iteration some steps are performed. Each of these iterations is also called epoch.\nIt is to be noted that, before entering into the loop, all the parameters are randomly initialised.\nBelow are the steps that are performed within a training loop.\n\n\n\n\n\nflowchart TD\n    A(randomly initialize&lt;br&gt;parameters) --&gt; B(compute output&lt;br&gt;for all the nodes&lt;br&gt;in each of the layers)--&gt;|compare with&lt;br&gt;ground truth|C(compute loss) --&gt; D(perform&lt;br&gt;backpropagation) --&gt; E(update&lt;br&gt;parameters) --&gt; B\n\n    subgraph forward pass\n    B\n    C\n    end\n\n    subgraph backward pass\n    D\n    end\n\n    subgraph update\n    E\n    end"
  },
  {
    "objectID": "posts/2023-05-01-embeddings/index.html",
    "href": "posts/2023-05-01-embeddings/index.html",
    "title": "Make More Sense of Language Through Embeddings",
    "section": "",
    "text": "Photo by Mel Poole on Unsplash"
  },
  {
    "objectID": "posts/2023-05-01-embeddings/index.html#one-hot-vector",
    "href": "posts/2023-05-01-embeddings/index.html#one-hot-vector",
    "title": "Make More Sense of Language Through Embeddings",
    "section": "One-hot vector",
    "text": "One-hot vector\nThis is the simplest way to convert tokens into a vector. One-hot vector, for a token, is a vector of dimension vocabulary_size (often denoted by |V|). In this vector, all the elements are 0‚Äôs except a single 1. The index that holds the 1 is called hot index and it is basically the index of the token in the vocabulary.\nExample:\n\none_hot_for_around = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\none_hot_for_can = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\none_hot_for_number = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n\nAs you can imagine, for a large corpus, one-hot vectors are very large dimensional sparse vectors.\nAlso, if we represent any text using one-hot vectors (technically a matrix made of one-hot vectors), then basically we are not considering word-word relationship or similarity.\nWords (rather tokens) are considered independent chunks of the text for this kind of setup, but it is something that we do not like. Any language maintains some contexts through word-word relationships and those are completely ignored by one-hot representation.\nIf we try to measure the similarity between two words with the help of one-hot representation, we will always get zero, but that does not mean the words are very close on the semantic space."
  },
  {
    "objectID": "posts/2023-05-01-embeddings/index.html#embedding-vector",
    "href": "posts/2023-05-01-embeddings/index.html#embedding-vector",
    "title": "Make More Sense of Language Through Embeddings",
    "section": "Embedding vector",
    "text": "Embedding vector\nAs you have observed, one-hot vectors do not consider context, so we definitely need something meaningful for representing the tokens mathematically.\nIn the world of NLP, the term embedding usually refers to a dense vector (a vector that has most of its components as non-zero real numbers) that represents a token. As you know, depending on the problem, a token can be a character, a word or even a full sentence and an embedding vector is created to represent it accordingly.\nLet us try to understand the way of expressing the meaning of words mathematically.\n\n\n\nFigure 1: Word features representation\n\n\nIn the above table, we have four different words which are ‚ÄòChocolate‚Äô, ‚ÄòCat‚Äô, ‚ÄòDog‚Äô and ‚ÄòShip‚Äô. For each of these words we have six randomly chosen features or characteristics.\nEach of the features can have values in between zero and one. If the value for a feature is very close to zero, it indicates that the feature is not very applicable to the word. Similarly, having a value closer to one indicates that the feature is quite applicable to the word.\nAs an example, the feature ‚Äòcan_play_with‚Äô is not at all relevant to the word ‚ÄòShip‚Äô, that‚Äôs why the score is 0.0.\nInteresting thing to note here is that the words ‚ÄòCat‚Äô and ‚ÄòDog‚Äô have very similar scores for all the features. This makes them very similar words and in reality, they do have many common characteristics too.\nIf we consider a 6D vector space (also called semantic space) based on these features, then obviously, ‚ÄòCat‚Äô and ‚ÄòDog‚Äô are going to be very close to each other over there. However, the words ‚ÄòChocolate‚Äô and ‚ÄòShip‚Äô will be far away.\nThis is how, by using vectors, we can mathematically express the meaning of words to a computer. These vectors are known as embedding vectors.\nAlthough embedding vectors are very helpful, they are very difficult to design. We do not really know how to design the features."
  },
  {
    "objectID": "posts/2023-05-01-embeddings/index.html#learning-word-embeddings",
    "href": "posts/2023-05-01-embeddings/index.html#learning-word-embeddings",
    "title": "Make More Sense of Language Through Embeddings",
    "section": "Learning word embeddings",
    "text": "Learning word embeddings\nNeural networks, as versatile tools, come here to solve the problem. Here, we will learn an algorithm, called word2vec that helps us in getting embedding vectors by using a shallow (not very deep) neural network trained on some texts.\n\n\n\n\n\n\nTip\n\n\n\nBefore delving into the algorithm, we must understand one simple yet very useful application of one-hot vector and matrix multiplication.\nSuppose, we have an embedding matrix (made of stacking the embedding vectors together) \\mathbf{E}^T_{4 \\times 6} just like the above figure.\nIf we consider our vocabulary as ['Cat', 'Dog', 'Chocolate', 'Ship'], then one-hot vector for the word ‚ÄòDog‚Äô will be \\vec{v}_{Dog} = \\left(0, 1, 0, 0\\right).\nNow, if we multiply \\vec{v}_{Dog} and \\mathbf{E}^T, then the result of \\vec{v}_{Dog} \\cdot \\mathbf{E}^T gives us the second row of \\mathbf{E}^T which is nothing but the embedding vector of the word ‚ÄòDog‚Äô.\nTherefore, we can use an on-hot vector and extract the corresponding embedding vector from the embedding matrix. This is why embedding matrices are also called lookup tables.\n\n\nIf we consider a word in a sentence, then it is very obvious to note that the surrounding words have some relationships with it.\nAs an example, if we have a sentence like ‚Äòthe dog is swimming in the pond‚Äô, then the words swimming and pond are related.\nThe word swimming is the context here, if we know this, we can anticipate other words similar to pond as well just by following the context.\n\nContinuous bag of words (CBOW) model\nConsider an incompelte sentence: the birds are _____ in the sky. By looking at the words the, birds, are, on, the, sky, can we imagine the missing word? Ofcourse, it must be either flying or very similar to it.\nThis is exactly what happens when we use the CBOW approach. CBOW looks at the context and tries to figure out the target word as shown below,\n\n\n\nFigure 2: Predicting target word based on context\n\n\nIn reality, this context window moves towards the right till the time we have covered all possible contexts and targets starting from the beginning of our text data to the end. Don‚Äôt worry, the code example, given below, will make this easier to understand.\nWe will now consider a sample text and prepare the training data using CBOW approach for a shallow feed-forward network. It‚Äôs going to have a single hidden layer. The number of neurons in the hidden layer is controlled by the dimension of the embedding vector.\nA pseudo network architecture is shown below,\n\n\n\nFigure 3: A pseudo CBOW network\n\n\nIn this network, each word in the context is fed to the network as an one-hot vector. Having the input vector, the network does some computations to form the hidden layer and eventually tries to predict the target word. The source code depicting the network architecture is given below.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Word2Vec(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(Word2Vec, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # define weight matrices\n        self.weight_i2h = nn.Parameter(torch.randn(\n            (self.embedding_dim, self.vocab_size), \n            dtype=torch.float\n        ))\n        self.weight_h2o = nn.Parameter(torch.randn(\n            (self.vocab_size, self.embedding_dim), \n            dtype=torch.float\n        ))\n\n        self.params = nn.ParameterList(\n            [\n                self.weight_i2h, \n                self.weight_h2o \n            ]\n        )\n\n    def forward(self, input):\n        # create onehot vector of the input\n        input_onehot = F.one_hot(input, self.vocab_size)\n\n        # compute hidden layer\n        hidden = F.relu(\n            torch.matmul(\n                self.weight_i2h, \n                input_onehot.view(self.vocab_size, -1).float()\n            )\n        )\n\n        # compute output\n        output = torch.matmul(\n            self.weight_h2o,\n            hidden\n        )\n\n        # compute log softmax\n        log_probs = F.log_softmax(output.view(1, -1), dim=1)\n\n        return log_probs\n\nNote the output dimension of the network. It is exactly equal to the size of the vocabulary as there are that many number of possibilities while predicting the target word.\nSo, essentially, this whole process is a multi-class classification process.\nIt is strange to know that prediction is not what we actually focus on here. We are interested in the weight matrix \\mathbf{E}. This is our embedding matrix filled with optimally trained parameters.\n\\mathbf{E} has the dimension dim(\\text{embedding vector}) \\times dim(\\text{one-hot vector}).\nHence, \\vec{v}_{word_j} \\cdot \\mathbf{E}^T just gives us the embedding vector for word_j.\nLet us now see how the training data is prepared for the CBOW network from a sample text.\n\n# imports\nimport re\nfrom pprint import pprint\n\nraw_text = \"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\"\n\n# clean raw text\ncleaned_text = re.sub(r\"[\\.\\,\\-]\", \" \", raw_text).lower().replace('\\n',\"\")\ndata = cleaned_text.lower().split();\n\n# source: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n\nWe‚Äôll now prepare the training data using the code below,\n\n# specify context size and embedding dimension\nCONTEXT_SIZE = 2 # two words on the left and two words on the right\nEMBEDDING_DIM = 10\n\n# create vocabulary\nsorted_vocab = sorted(list(set(data)))\n\n# prepare dataset for cbow network\ncbow_data = []\n\n# define start and end indices\nstart_ix = CONTEXT_SIZE; end_ix = len(data) - CONTEXT_SIZE\n\nfor i in range(start_ix, end_ix):\n    target = data[i]\n    for j in range((i - CONTEXT_SIZE), (i + CONTEXT_SIZE + 1)):\n        if j != i:\n            cbow_data.append((data[j], target))\n\nand here is the partial training data as the output,\n\npprint(cbow_data[:10])\n\n[('we', 'about'),\n ('are', 'about'),\n ('to', 'about'),\n ('study', 'about'),\n ('are', 'to'),\n ('about', 'to'),\n ('study', 'to'),\n ('the', 'to'),\n ('about', 'study'),\n ('to', 'study')]\n\n\n\nSkip-gram model\nSkip gram, being another variation of word2vec, does the exact opposite of what CBOW does.\nIt tries to predict the context or surrounding words by looking at a specific word in a sentence as shown below,\n\n\n\nFigure 4: Predicting context based on target word\n\n\nHere is the code example that helps to prepare the training data based on the same raw text,\n\n# prepare dataset for cbow network\nskip_gram_data = []\n\n# define start and end indices\nstart_ix = CONTEXT_SIZE; end_ix = len(data) - CONTEXT_SIZE\n\nfor i in range(start_ix, end_ix):\n    target = data[i]\n    for j in range((i - CONTEXT_SIZE), (i + CONTEXT_SIZE + 1)):\n        if j != i:\n            skip_gram_data.append((target, data[j]))\n\nand below is the partial training data as output,\n\npprint(skip_gram_data[:10])\n\n[('about', 'we'),\n ('about', 'are'),\n ('about', 'to'),\n ('about', 'study'),\n ('to', 'are'),\n ('to', 'about'),\n ('to', 'study'),\n ('to', 'the'),\n ('study', 'about'),\n ('study', 'to')]\n\nCBOW and Skip-Gram training data look very similar, the only difference is in the usage of target and context words in the training data.\nThe source code for training such a network is given in this colab notebook. It may not be optimised, but it will give you enough confidence to understand the whole story behind word2vec."
  },
  {
    "objectID": "posts/2023-05-01-embeddings/index.html#pre-trained-embedding-models",
    "href": "posts/2023-05-01-embeddings/index.html#pre-trained-embedding-models",
    "title": "Make More Sense of Language Through Embeddings",
    "section": "Pre-trained embedding models",
    "text": "Pre-trained embedding models\nTo get very accurate vector representations of words, we need to train such networks on very large corpus with a much larger embedding dimension.\nResearchers have come up with pre-trained embedding models which are trained on massive amounts of text such as the wikipedia. These models are typically of 300 to 600 dimensional and are ready to be consumed.\nThe only problem with pre-trained models is the training data is very generic. In case someone needs to work on a specific domain, then such a network should be trained on domain specific corpus."
  },
  {
    "objectID": "posts/2023-03-19-stat-hypothesis-test/index.html",
    "href": "posts/2023-03-19-stat-hypothesis-test/index.html",
    "title": "Testing of Hypotheses: A Quick Refresher",
    "section": "",
    "text": "Photo by Susan Holt Simpson on Unsplash"
  },
  {
    "objectID": "posts/2023-03-19-stat-hypothesis-test/index.html#some-real-life-examples",
    "href": "posts/2023-03-19-stat-hypothesis-test/index.html#some-real-life-examples",
    "title": "Testing of Hypotheses: A Quick Refresher",
    "section": "Some Real Life Examples",
    "text": "Some Real Life Examples\nI will try to relate the definitions with couple of examples:\n\nSuppose the Honorable judge is about to give his verdict to the accused. Now let‚Äôs assume that our null hypothesis or prior belief about that person is, the person is not guilty. If the judge‚Äôs verdict is against the accused even if the person has really not done anything wrong, then it is the Type-I error, whereas if the verdict is in favor of the accused when in reality the person has done something wrong, then it is the Type-II error. If you‚Äôd think for a moment, you would understand the fact that Type-I error is more severe in nature.\nThis one is going to be of great impotance for the society. We know that almost every country has their own military radar system to detect any missilies aimed at them. Assume that, the null hypothesis is the enemy is in war mode, they will attack. Now, could you imagine the situation, if the military radar system would have failed to detect the missiles that had been aimed at your country and they were few miles away from their target‚Ä¶the lives of innocent citizens would have been devastated.\n\n\n\nfig-1\n\n\n\nHaving understood the importance of Type-I error in decision making, the usual tendency of the experimenter is to keep the chance of making Type-I error fixed at a very low level, which, by years of research and experimentations, has been chosen as 1% or 5%, depending on the situation."
  },
  {
    "objectID": "posts/2023-03-22-bash-commands-for-cloud-1/index.html",
    "href": "posts/2023-03-22-bash-commands-for-cloud-1/index.html",
    "title": "Bash Commands to Get Started on Linux Clouds",
    "section": "",
    "text": "Photo by C Dustin on Unsplash"
  },
  {
    "objectID": "posts/2023-03-22-bash-commands-for-cloud-1/index.html#getting-the-basic-information",
    "href": "posts/2023-03-22-bash-commands-for-cloud-1/index.html#getting-the-basic-information",
    "title": "Bash Commands to Get Started on Linux Clouds",
    "section": "Getting the basic information",
    "text": "Getting the basic information\n\nüëâ The whoami command\nwhoami is a command that returns the user name currently logged in.\n!whoami\nkoushik\n\n\nüëâ The date command\ndate gives you the current date and time.\n!date\nWed Mar 22 17:23:23 IST 2023\n\n\nüëâ The pwd command\npwd stands for the present working directory which is pointed out by the terminal. For me the pwd is /home/koushik/\n!pwd\n/home/koushik/quarto-docs\n\n\nüëâ The ls command\nls stands for listing, which returns names of all the files and directories (folders) availble inside the pwd.\n!ls\nbash-commands-for-cloud.ipynb  koushik.txt     records.txt\nbash-commands-for-cloud.md     my_first_file.txt   test_dir\nbash-commands-for-cloud.qmd    my_second_file.txt\nNote that ls only returns the names of files and directories which are not hidden, this is the default behavior of ls, if you want to see something more you need some other options while using it, which are often called flags.\nI will use some flags below and try to explain what they mean.\ndifferent flags of ls\n!ls -a\n.                  bash-commands-for-cloud.qmd  records.txt\n..                 koushik.txt          test_dir\nbash-commands-for-cloud.ipynb  my_first_file.txt\nbash-commands-for-cloud.md     my_second_file.txt\n-a flag is used for showing all the files and directories within pwd including hidden ones. In linux hidden files are directories have . in their names at the the very beginning.\n!ls -h\nbash-commands-for-cloud.ipynb  koushik.txt     records.txt\nbash-commands-for-cloud.md     my_first_file.txt   test_dir\nbash-commands-for-cloud.qmd    my_second_file.txt\n-h flag is used for better representation for the users (human being, that‚Äôs why h)\n!ls -l\ntotal 68\n-rw-r--r-- 1 koushik koushik 16787 Mar 22 17:23 bash-commands-for-cloud.ipynb\n-rw-r--r-- 1 koushik koushik 17507 Mar 22 17:13 bash-commands-for-cloud.md\n-rw-r--r-- 1 koushik koushik  9697 Mar 22 17:23 bash-commands-for-cloud.qmd\n-rw-r--r-- 1 koushik koushik    58 Mar 22 12:58 koushik.txt\n-rw-r--r-- 1 koushik koushik     0 Mar 22 17:13 my_first_file.txt\n-rw-r--r-- 1 koushik koushik    58 Mar 22 12:49 my_second_file.txt\n-rw-r--r-- 1 koushik koushik    87 Mar 22 17:13 records.txt\ndrwxr-xr-x 2 koushik koushik  4096 Mar 22 17:12 test_dir\n-l flag is used for showing entries in a long format\n!ls -t\nbash-commands-for-cloud.ipynb  records.txt    koushik.txt\nbash-commands-for-cloud.qmd    my_first_file.txt  my_second_file.txt\nbash-commands-for-cloud.md     test_dir\n-t flag is used for showing entries sorted based on when they are created\n!ls -s\ntotal 68\n20 bash-commands-for-cloud.ipynb   4 koushik.txt      4 records.txt\n20 bash-commands-for-cloud.md      0 my_first_file.txt    4 test_dir\n12 bash-commands-for-cloud.qmd     4 my_second_file.txt\n-s flag is used for showing the allocated sizes of the files\nYou can definitely combine multiple flags together just by putting them next to each other like below and obviously you will get the combined effect of them\n!ls -lahts\ntotal 76K\n 20K -rw-r--r--  1 koushik koushik  17K Mar 22 17:23 bash-commands-for-cloud.ipynb\n4.0K drwxr-xr-x  3 koushik koushik 4.0K Mar 22 17:23 .\n 12K -rw-r--r--  1 koushik koushik 9.5K Mar 22 17:23 bash-commands-for-cloud.qmd\n 20K -rw-r--r--  1 koushik koushik  18K Mar 22 17:13 bash-commands-for-cloud.md\n4.0K -rw-r--r--  1 koushik koushik   87 Mar 22 17:13 records.txt\n   0 -rw-r--r--  1 koushik koushik    0 Mar 22 17:13 my_first_file.txt\n4.0K drwxr-xr-x  2 koushik koushik 4.0K Mar 22 17:12 test_dir\n4.0K -rw-r--r--  1 koushik koushik   58 Mar 22 12:58 koushik.txt\n4.0K -rw-r--r--  1 koushik koushik   58 Mar 22 12:49 my_second_file.txt\n4.0K drwxr-xr-x 14 koushik koushik 4.0K Mar 22 12:39 ..\nOne more thing to note here is that, you can use ls to see the contents of any directory just by putting the path next to ls call followed by a space character like below\n!ls -lahts /\ntotal 2.0M\n4.0K drwxrwxrwt   8 root root 4.0K Mar 22 17:23 tmp\n4.0K drwxr-xr-x  80 root root 4.0K Mar 22 10:44 etc\n4.0K drwxr-xr-x   4 root root 4.0K Mar 22 10:44 opt\n4.0K drwx------   3 root root 4.0K Mar 22 10:44 root\n   0 drwxr-xr-x   7 root root  140 Mar 22 08:52 run\n4.0K drwxr-xr-x  19 root root 4.0K Mar 22 08:50 .\n4.0K drwxr-xr-x  19 root root 4.0K Mar 22 08:50 ..\n   0 drwxr-xr-x  11 root root 3.0K Mar 22 08:50 dev\n   0 dr-xr-xr-x 202 root root    0 Mar 22 08:50 proc\n   0 dr-xr-xr-x  11 root root    0 Mar 22 08:50 sys\n4.0K drwxr-xr-x   3 root root 4.0K Mar 20 19:58 home\n4.0K drwxr-xr-x   5 root root 4.0K Mar 20 19:57 mnt\n 16K drwx------   2 root root  16K Mar 20 19:57 lost+found\n4.0K drwxr-xr-x   8 root root 4.0K Feb 11 03:06 snap\n4.0K drwxr-xr-x  13 root root 4.0K Feb 11 03:06 var\n4.0K drwxr-xr-x  14 root root 4.0K Feb 11 03:05 usr\n4.0K drwxr-xr-x   2 root root 4.0K Feb 11 03:05 media\n4.0K drwxr-xr-x   2 root root 4.0K Feb 11 03:05 srv\n   0 lrwxrwxrwx   1 root root    7 Feb 11 03:05 lib -&gt; usr/lib\n   0 lrwxrwxrwx   1 root root    9 Feb 11 03:05 lib32 -&gt; usr/lib32\n   0 lrwxrwxrwx   1 root root    9 Feb 11 03:05 lib64 -&gt; usr/lib64\n   0 lrwxrwxrwx   1 root root   10 Feb 11 03:05 libx32 -&gt; usr/libx32\n   0 lrwxrwxrwx   1 root root    8 Feb 11 03:05 sbin -&gt; usr/sbin\n   0 lrwxrwxrwx   1 root root    7 Feb 11 03:05 bin -&gt; usr/bin\n4.0K drwxr-xr-x   2 root root 4.0K Apr 18  2022 boot\n1.9M -rwxrwxrwx   1 root root 1.9M Jan  1  1970 init\nHere ls is showing the contents of a special directory a.k.a root, this is equivalent to the C:\\ drive on Windows.\n\n\nüëâ The echo command\necho evaluates an expression and prints that on the terminal.\n!echo $(date)\nWed Mar 22 17:23:24 IST 2023\nhere date is evaluated by echo and the output of echo is printed on the terminal (or console).\nHere is another example of echo.\n!name=\"Julia\" && \\\necho \"$name claims to be faster that Python\"\nJulia claims to be faster that Python\nthere are two things to note here:\n\nwe are creating a shell variable called name with the value Julia and this variable is referred in the echo call to print a formatted string on the console\ncreation of name and calling echo are two separate commands which are being executed in a sequence by using && operator. The \\ is used for breaking the lines to make the command flow through multiple lines."
  },
  {
    "objectID": "posts/2023-03-22-bash-commands-for-cloud-1/index.html#working-with-files-and-directories",
    "href": "posts/2023-03-22-bash-commands-for-cloud-1/index.html#working-with-files-and-directories",
    "title": "Bash Commands to Get Started on Linux Clouds",
    "section": "Working with files and directories",
    "text": "Working with files and directories\nNow, you know the basics of running commands and getting some simple yet useful information, it is the time to see a bit more interesting commands.\n\nüëâ Creating a file with touch\n!touch my_first_file.txt\ntouch creates a file with the name given by the user right next to it followed by a space character.\n\n\nüëâ Writing data to a file using tee\n# writing date to file, the file will be created if it does not exist\n!date | tee records.txt\n\n# writing data in append mode with flag '-a'\n!date | tee -a records.txt\n\n# evaluate and then write in append mode\n!echo $(date) | tee -a records.txt\nWed Mar 22 17:23:25 IST 2023\n\nWed Mar 22 17:23:25 IST 2023\n\nWed Mar 22 17:23:25 IST 2023\nBy default, tee overwrites the data to the file, -a is used to avoid this.\nWe have one more operator (|) here, it is called pipe operator. It takes the output of the previous command and passes to the next command.\nWriting same data to multiple files is also very intuitive as in date | tee -a file1.txt file2.txt file3.txt\nThe same operations can also be performed using redirect operator &gt;. date &gt; record.txt will write the system date time by overrwriting the file and date &gt;&gt; record.txt will write the data to the file in append mode.\nA file can be removed using the rm command, e.g.¬†rm record.txt\n\n\nüëâ Working with directory (folder)\n# create directory with 'mkdir'\n!mkdir test_dir_1\n!mkdir test_dir_2\n\n# create directory recussively along a path with flag '-p'\n!mkdir -p test_dir_1/sub_dir/sub_sub_dir\n\n# remove directories with 'rm'\n!rm -d test_dir_2 # '-d' flag (directory) is used to remove an empty directory\n!rm -r test_dir_1 # '-r' flag (recurive) is used to remove a directory and all of its contents\n# rm -rf test_dir_1 '-f' flag (force) is used to remove a directory and all of its contents forcefully\nmultiple directories can also be removed just by specifying their paths one by one. One very useful flag for rm command is -i, which makes the removal process interactive.\nNote: In linux, if you add a . before the file and diretory name, then it will be hidden which means simple ls command cannot show you these, you must to use ls -a to locate them üòâ .\n\n\nüëâ Copying files and directories from source to destination\nTo copy a file or directory from source to destination, we have the command cp. See the examples below to understand how it works,\n\ncopy a file: cp source_path/file.txt destination_path/\ncopy a directory: cp -r source_path/test_dir/ destination_path/\n\n\n\nüëâ Searching for files and directories using grep\nSometimes you may need a search for a file or directory inside the terminal. This can be done using the command called grep. It is used with ls through a pipe operator as if we are telling bash to filter the ls output by the pattern. See the examples below,\n# will only show files and directories with the word 'first' in their names \n!ls -lahts | grep first\n   0 -rw-r--r--  1 koushik koushik    0 Mar 22 17:23 my_first_file.txt\n\n\nüëâ Editing a file with nano\nnano is a text editor which typically available in almost all linux based systems. It is very useful if you quickly want to edit a file without leaving the terminal.\nnano &lt;your_file_name&gt; is the command to open a file with nano. Once you are done with the editing, you can use Ctrl+S (Cmd+S in case you‚Äôre using clound from a Mac) to save the file and Ctrl+X (or Cmd+X in Mac) to close the editor.\n\n\nüëâ Viewing the content of a file using cat\nIn case you want to see the content of a file, you have a command called cat. It has several flags to use based on your requirements. Be careful, when using cat for a large file, by default it will show you everything that is there in the file.\ncat records.txt\nWed Mar 22 17:23:25 IST 2023\nWed Mar 22 17:23:25 IST 2023\nWed Mar 22 17:23:25 IST 2023\nYou can use the flag -n to see the line numbers right before the lines.\ncat -n records.txt\n     1  Wed Mar 22 17:23:25 IST 2023\n     2  Wed Mar 22 17:23:25 IST 2023\n     3  Wed Mar 22 17:23:25 IST 2023\nThat‚Äôs all for the part 1. I hope you have enjoyed reading the post, stay tuned for the next part."
  },
  {
    "objectID": "posts/2023-03-26-bash-commands-for-cloud-2/index.html",
    "href": "posts/2023-03-26-bash-commands-for-cloud-2/index.html",
    "title": "Bash Commands to Get Started on Linux Clouds",
    "section": "",
    "text": "Photo by C Dustin on Unsplash\nIn our previous post, we have seen examples to become confident enough for getting a grip on linux commands. This is the time now to jump into more advanced topics. Let‚Äôs go."
  },
  {
    "objectID": "posts/2023-03-26-bash-commands-for-cloud-2/index.html#permission-in-numeric-mode",
    "href": "posts/2023-03-26-bash-commands-for-cloud-2/index.html#permission-in-numeric-mode",
    "title": "Bash Commands to Get Started on Linux Clouds",
    "section": "Permission in numeric mode",
    "text": "Permission in numeric mode\nAll these three characters i.e.¬†‚Äòr‚Äô, ‚Äòw‚Äô and ‚Äòx‚Äô are also represented by numbers as follows:\n\nr (read): 4\nw (write): 2\nx (execute): 1\n\nand their sum is expressed as the desired permission for an user. Some examples below will make the concept clear.\n\nr = 4, w = 2, x = 1\n\nsum = 7\nthe user has read, write and execute permission for the corresponding file\n\nr = 4, w = 2, x = 0\n\nsum = 6\nthe user has read and write permission only for the corresponding file, but the file cannot be executed as a program\n\nr = 4, w = 0, x = 1\n\nsum = 5\nthe user has read and execute permission only for the corresponding file, but the file cannot be modified"
  },
  {
    "objectID": "bookmarks.html",
    "href": "bookmarks.html",
    "title": "Some Useful Bookmarks",
    "section": "",
    "text": "I have tried here to maintain some useful bookmarks that have been of great help to me in understanding various concepts. I hope it will be useful for the reader too.\n\nA friendly introduction to time series analysis: Website\nUnderstanding the LSTM Networks: Blog Post\nOne of the best resources to setup your own portfolio using Quarto and GitHub Actions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Koushik Khan",
    "section": "",
    "text": "Hello, myself Koushik Khan. I work as Principal Data Scientist at S&P Global Market Intelligence, Bangalore, India.\nI have close to nine years of industry experience in working with various analytical problems while helping stakeholders make informed decisions.\nThis is my personal portfolio where I maintain my career details along with publishing my learnings and experiences in the form of blog posts.\nThe list below is to help you navigate through the site.\n\n Home: This is where you are at right now\n Blog: List of blog posts that I‚Äôve written\n Projects: Links to my personal projects\n Bookmarks: Links to some helpful books/articles\n Skills: A curated list of projects I have delivered\n Resume: My latest resume is attached here\n About: About myself\n\nThanks for taking some time to visit my portfolio."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "B.Sc. in Statistics (2013) - Visva Bharati University, India\nM.Sc. in Statistics (2015) - Visva Bharati University, India"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "B.Sc. in Statistics (2013) - Visva Bharati University, India\nM.Sc. in Statistics (2015) - Visva Bharati University, India"
  },
  {
    "objectID": "about.html#professional-career-history",
    "href": "about.html#professional-career-history",
    "title": "About",
    "section": " Professional career history",
    "text": "Professional career history\n\nBCausE Enterprise Pvt. Ltd.¬†as a Junior Data Analyst (2015 - 2016)\nIBM India Pvt. Ltd.¬†as a Data Scientist (2016 - 2021)\nS&P Global Inc.¬†as a Sr.¬†Data Scientist (2021 - Present)"
  },
  {
    "objectID": "about.html#current-job-responsibilities",
    "href": "about.html#current-job-responsibilities",
    "title": "About",
    "section": " Current job responsibilities",
    "text": "Current job responsibilities\n\nAnalysing maritime data\nProduct development and maintenance\nLeading a team to build data pipelines for consuming data from external sources"
  },
  {
    "objectID": "about.html#things-i-like-to-do-apart-from-work",
    "href": "about.html#things-i-like-to-do-apart-from-work",
    "title": "About",
    "section": " Things I like to do apart from work",
    "text": "Things I like to do apart from work\n\nTravelling to historical places\nReading books, especially Bengali Novels\nWriting articles on something interesting I learn\nTeaching\nLearning and experimenting with different ideas and tools\nCooking - thanks to Covid to help me in discovering this skill"
  }
]